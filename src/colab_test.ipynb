{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "colab_test.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "POJfkUeHELGD",
        "colab_type": "code",
        "outputId": "bece8140-b9b5-4e35-aa56-205499883be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWk4rHFeGXuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = '/content/drive/My Drive/proto_data/'\n",
        "PICKLED_DIR = os.path.join(DATA_DIR, 'pickled/')\n",
        "PREDICTIONS_DIR = os.path.join(DATA_DIR, 'model_predictions/')\n",
        "#CONLLU_DIR = os.path.join(DATA_DIR, 'WSJ_conllus/')\n",
        "#MODEL_DIR = '../saved_models/'\n",
        "\n",
        "PROTO_TSV = os.path.join(DATA_DIR, 'protoroles_eng_pb_08302015.tsv')\n",
        "#GLOVE_FILE = {'100': os.path.join(DATA_DIR, 'glove.6B.100d.txt') }\n",
        "\n",
        "SPLITS = ['train', 'dev', 'test'] \n",
        "\n",
        "NUM_DEP_RELS = 6\n",
        "\n",
        "PROPERTIES = ['instigation', 'volition', 'awareness', 'sentient',\n",
        "'exists_as_physical', 'existed_before', 'existed_during', 'existed_after',\n",
        "'created', 'destroyed', 'predicate_changed_argument', 'change_of_state', \n",
        "'changes_possession', 'change_of_location', 'stationary', 'location_of_event', \n",
        "'makes_physical_contact', 'manipulated_by_another']\n",
        "\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "weights_path = '/content/drive/My Drive/proto_data/weights.tch'\n",
        "original_weights_path = '/content/drive/My Drive/proto_data/original_weights.tch'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUBxnzAlVE9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/drive/My Drive/proto_modules')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50wGdMdn8tTO",
        "colab_type": "code",
        "outputId": "fbfe69b0-efa2-44c4-89d4-2be9f503f3bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD, AdamW, LBFGS\n",
        "import scipy as sp\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device: ', device)\n",
        "\n",
        "#import data_utils"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device:  cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4IAS8EuYvxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import unsqueeze\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, \\\n",
        "        pack_padded_sequence, pad_sequence\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self,\n",
        "               n_properties=None,\n",
        "               specific_size=None):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.n_properties = n_properties\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(self.n_properties * specific_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = x.shape[0]\n",
        "\n",
        "    x = x.view(B, self.n_properties, -1) # (B, n_props, size_specific)\n",
        "\n",
        "    attn_scores = torch.bmm(x, torch.transpose(x, 2, 1)) # (B, n_props, n_props)\n",
        "    dists = F.softmax(attn_scores, -1)\n",
        "    attn_weighted_sum = torch.bmm(dists, x)\n",
        "\n",
        "    x = x + attn_weighted_sum\n",
        "\n",
        "    x = self.layer_norm(x.view(B, -1))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class SPRL(nn.Module):\n",
        "    def __init__(self,\n",
        "            vocab_size=None,\n",
        "            emb_size=None,\n",
        "            h_size=None,\n",
        "            shared_size=None,\n",
        "            padding_idx=None,\n",
        "            emb_np=None,\n",
        "            properties=None,\n",
        "            use_attention=False,\n",
        "            use_lstm=False,\n",
        "            direction_feature=False,\n",
        "            #train_jointly=False,\n",
        "            eye=False,\n",
        "            updates_are_logits=False,\n",
        "            sent_avg=False,\n",
        "            use_shared_rep=False,\n",
        "            use_dep_rels=False,\n",
        "            use_updates=False):\n",
        "        super(SPRL, self).__init__()\n",
        "\n",
        "        self.properties = properties\n",
        "        self.n_properties = len(properties)\n",
        "        self.direction_feature = direction_feature\n",
        "        #self.train_jointly = train_jointly\n",
        "        self.use_shared_rep = use_shared_rep\n",
        "        self.use_dep_rels = use_dep_rels\n",
        "        self.use_updates = use_updates\n",
        "\n",
        "        self.word_emb = nn.Embedding(\n",
        "                vocab_size,\n",
        "                emb_size,\n",
        "                padding_idx=padding_idx)\n",
        "        self.word_emb.weight.data.copy_(torch.Tensor(emb_np))\n",
        "        self.word_emb.weight.requires_grad = False\n",
        "\n",
        "        self.lstm = None\n",
        "        if use_lstm:\n",
        "          self.use_lstm = True\n",
        "          directions = 2\n",
        "          self.lstm = MyLSTM(\n",
        "              emb_size=emb_size,\n",
        "              h_size=h_size,\n",
        "              directions=directions)\n",
        "\n",
        "          concatenated_embs_size = 2*(directions*h_size)\n",
        "        \n",
        "        else:\n",
        "          concatenated_embs_size = (2*emb_size) + int(direction_feature)\n",
        "          self.sent_avg = sent_avg\n",
        "          if self.sent_avg:\n",
        "            concatenated_embs_size += emb_size # While testing pred, head, and average sent embedding\n",
        "        if self.use_dep_rels:\n",
        "          concatenated_embs_size += NUM_DEP_RELS\n",
        "\n",
        "        # self.attention = None\n",
        "        # if not use_attention:\n",
        "        shared_size = concatenated_embs_size\n",
        "        #shared_size = self.n_properties\n",
        "        if self.use_shared_rep:\n",
        "          self.shared = nn.Sequential(\n",
        "              nn.Linear(concatenated_embs_size, shared_size, bias=True),\n",
        "              nn.ReLU(),\n",
        "              )\n",
        "          self.prop_specific = nn.Sequential(\n",
        "              nn.Linear(shared_size, self.n_properties, bias=True),\n",
        "          )\n",
        "        else:\n",
        "          print('\\nNO shared representation!\\n')\n",
        "          self.shared = lambda x: x # identity func\n",
        "          self.prop_specific = nn.Sequential(\n",
        "              nn.Linear(concatenated_embs_size, self.n_properties, bias=True),\n",
        "          )\n",
        "        \n",
        "        # else: \n",
        "        #   self.attention = True\n",
        "        #   self.first_guess = nn.Sequential(\n",
        "        #       nn.Linear(concatenated_embs_size, self.n_properties, bias=True),\n",
        "        #       )\n",
        "        \n",
        "        self.updates= None\n",
        "        self.eye = eye\n",
        "        if self.use_updates:\n",
        "          self.updates_are_logits = updates_are_logits \n",
        "          \n",
        "          self.updates = nn.Linear(self.n_properties, self.n_properties, bias=False)\n",
        "          \n",
        "\n",
        "          if self.eye:\n",
        "            self.updates.weight.data -= (torch.eye(self.n_properties) * self.updates.weight.data)\n",
        "\n",
        "\n",
        "    def forward(self, sents, sent_lens, preds, heads, rels, arg_begin_end, updates=True):\n",
        "\n",
        "        # Sort the sentences so that the LSTM can process properly\n",
        "        B, _, = sents.shape\n",
        "\n",
        "\n",
        "        if self.lstm != None: # i.e., use lstm\n",
        "          lens_sorted = sent_lens\n",
        "          sents_sorted = sents\n",
        "          indices = None\n",
        "          if(len(sents) > 1):\n",
        "              lens_sorted, indices = torch.sort(lens_sorted, descending=True)\n",
        "              lens_sorted = lens_sorted.to(device)\n",
        "              indices = indices.to(device)\n",
        "              sents_sorted = sents_sorted.index_select(0, indices).to(device)\n",
        "          w_embs = self.word_emb(sents_sorted)\n",
        "          packed_lstm_input = pack_padded_sequence(\n",
        "                  w_embs, lens_sorted, batch_first=True)\n",
        "\n",
        "          lstm_outs = self.lstm(packed_lstm_input, indices)\n",
        "\n",
        "          pred_reps = lstm_outs[np.arange(B), preds] # expecting (B, h_size)\n",
        "          head_reps = lstm_outs[np.arange(B), heads] # same as above\n",
        "        else: # no lstm, just MLP\n",
        "          w_embs = self.word_emb(sents)\n",
        "          pred_reps = w_embs[np.arange(B), preds] # expecting (B, h_size)\n",
        "          head_reps = w_embs[np.arange(B), heads] # same as above\n",
        "          if self.sent_avg:\n",
        "            sent_avg = w_embs.mean(dim=1)\n",
        "\n",
        "        pred_head_cat = torch.cat([pred_reps, head_reps], dim=-1)\n",
        "\n",
        "        if self.direction_feature:\n",
        "          head_before_pred = torch.unsqueeze((preds > heads), -1).float() # (B,)\n",
        "          pred_head_cat = torch.cat([pred_head_cat, head_before_pred], dim=-1) # (B, 2*h_size)\n",
        "          if self.sent_avg:\n",
        "            pred_head_cat = torch.cat([pred_head_cat, sent_avg], dim=-1) # (B, 2*h_size)\n",
        "        if self.use_dep_rels:\n",
        "          rels_one_hot = torch.zeros(B, NUM_DEP_RELS).to(device)\n",
        "          rels_one_hot.scatter_(1, rels.unsqueeze(-1), torch.ones(rels_one_hot.shape).to(device))\n",
        "          pred_head_cat = torch.cat([pred_head_cat, rels_one_hot], dim=-1) # (B, 2*h_size)\n",
        "\n",
        "        x = pred_head_cat\n",
        "        # if self.attention == None:\n",
        "        x = pred_head_cat # Experimenting with having no shared and learned rep\n",
        "        x = self.shared(x) # (B, size_shared) # For lstm\n",
        "\n",
        "        x = self.prop_specific(x)\n",
        "\n",
        "        # x = self.first_guess(x)\n",
        "        if updates and self.updates != None:\n",
        "          #x_up = self.updates(x)\n",
        "          x_up = self.updates((x.sign() + 1) / 2)\n",
        "\n",
        "          if self.updates_are_logits:\n",
        "            logits = x_up\n",
        "          else:\n",
        "            logits = x + x_up # Residual model\n",
        "        else:\n",
        "          logits = x\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def predict(self, logits):\n",
        "        # That is to say, predict 0 if logit < 0 and 1 if logit >= 0\n",
        "        predictions = (logits.sign() + 1) / 2\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "            emb_size=None,\n",
        "            h_size=None,\n",
        "            directions=None):\n",
        "        super(MyLSTM, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "                input_size=emb_size,    \n",
        "                hidden_size=h_size,\n",
        "                num_layers=1,\n",
        "                bidirectional=(directions == 2),\n",
        "                batch_first=True,\n",
        "                dropout=0.,\n",
        "                bias=True)\n",
        "        \n",
        "        self.lstm_drop = nn.Dropout(0.)\n",
        "\n",
        "    def forward(self, packed_lstm_input, indices):\n",
        "        outputs, _ = self.lstm(packed_lstm_input)\n",
        "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "        # Unsort sentences to return to proper alignment with labels\n",
        "        if len(outputs) > 1:\n",
        "            outputs = unsort(outputs, indices)\n",
        "          \n",
        "        outputs = self.lstm_drop(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        " \n",
        "def unsort(batch, indices):\n",
        "    indices_inverted = torch.argsort(indices)\n",
        "    batch = batch.index_select(0, indices_inverted)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahYg0bRx-Xdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(args, model, X, y):\n",
        "    epochs = args['epochs']\n",
        "    batch_size = args['batch_size']\n",
        "    lr = args['lr']\n",
        "\n",
        "    # Data loaders\n",
        "    loader_train = data_loader(X['train'], y['train'],\n",
        "            batch_size=batch_size, shuffle_idx=True)\n",
        "    n_train_batches = math.ceil(len(X['train']) / batch_size)\n",
        "    n_dev_batches = math.ceil(len(X['dev']) / batch_size)\n",
        "\n",
        "    if not args['train_jointly']: # Training prop-specific lstms or MLPs\n",
        "      torch.save(model.state_dict(), original_weights_path)\n",
        "\n",
        "      all_test_predictions = []\n",
        "      for prop_i, p in enumerate(PROPERTIES):\n",
        "        model.load_state_dict(torch.load(original_weights_path))\n",
        "\n",
        "        # Actual training happens here\n",
        "        model, training_losses, dev_losses, steps = train_loop(args, model, loader_train, n_train_batches, n_dev_batches, prop_i=prop_i)\n",
        "\n",
        "        # Evaluation starts here, best weights were returned as 'model' from train_loop\n",
        "        y_dev = get_prop_specific_labels(y['dev'], prop_i)\n",
        "        predictions = get_test_predictions(model, X['dev'], y_dev)\n",
        "        test_predictions = get_test_predictions(model, X['dev'], y_dev)\n",
        "        all_test_predictions.append(test_predictions)\n",
        "\n",
        "      all_test_predictions = torch.cat(all_test_predictions, dim=-1)\n",
        "    else:\n",
        "      model, training_losses, dev_losses, steps = train_loop(args, model, loader_train, n_train_batches, n_dev_batches)\n",
        "      all_test_predictions = get_test_predictions(model, X['dev'], y['dev'])\n",
        "\n",
        "    if args['model_name'] != '':\n",
        "      predictions_path = os.path.join(PREDICTIONS_DIR, args['model_name'])\n",
        "      if os.path.exists(predictions_path):\n",
        "        new_name = input(f\"Predictions for {args['model_name']} already exist. New name: \")\n",
        "        predictions_path = os.path.join(PREDICTIONS_DIR, new_name) # If same, means overwrite so OK\n",
        "      with open(predictions_path, 'wb') as f:\n",
        "        pickle.dump(all_test_predictions, f)\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=2, ncols=1)\n",
        "\n",
        "    ax[0].plot(np.arange(len(training_losses)), np.array(training_losses), color='orange')\n",
        "    ax[1].plot(np.arange(0, steps+1, n_train_batches), np.array(dev_losses), color='blue')\n",
        "    fig.show()\n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "def get_prop_specific_labels(y, prop_i):\n",
        "  y = [np.array([label[prop_i]]) for label in y]\n",
        "  return y\n",
        "\n",
        "\n",
        "def train_loop(args, model, loader_train, n_train_batches, n_dev_batches, prop_i=None):\n",
        "    epochs = args['epochs']\n",
        "    batch_size = args['batch_size']\n",
        "    lr = args['lr']\n",
        "    opt = Adam(model.parameters(), lr=lr, betas=[0.9, 0.999])\n",
        "\n",
        "    if args['show_weights']:\n",
        "      for name, param in model.named_parameters():\n",
        "        if 'updates' in name and 'weight' in name:\n",
        "          original_updates_weights = param.detach().clone()\n",
        "    \n",
        "    training_losses = []\n",
        "    dev_losses = [4] # Meaningless first number, to avoid calculating dev loss before training\n",
        "    steps = 0\n",
        "    cma = 0 # cumulative moving average of training loss\n",
        "    try:\n",
        "      prev_best = 0.0\n",
        "      phase_marker = 0\n",
        "      for e in range(epochs):\n",
        "        model.train()\n",
        "        steps_ = 0\n",
        "        stop_training_lower = e >= args['stop_training_lower']\n",
        "        if phase_marker == 0 and stop_training_lower:\n",
        "          phase_marker = 1\n",
        "\n",
        "        for b in tqdm(\n",
        "                range(n_train_batches), \n",
        "                ascii=True, \n",
        "                desc=f'Epoch {e+1}/{epochs} progress',\n",
        "                position=0,\n",
        "                leave=True,\n",
        "                ncols=80):\n",
        "          opt.zero_grad()\n",
        "          sents, sent_lens, preds, heads, rels, arg_begin_end, labels = next(loader_train)\n",
        "\n",
        "          if not args['train_jointly']:\n",
        "            labels = labels[:, prop_i].unsqueeze(-1)#\n",
        "\n",
        "          # if args['use_attention']:\n",
        "          if phase_marker == 1:\n",
        "            model.load_state_dict(torch.load(weights_path))\n",
        "            phase_marker = 2\n",
        "          \n",
        "          if stop_training_lower and args['use_updates']:              \n",
        "            for p in model.parameters():\n",
        "              p.requires_grad = False\n",
        "            for p in model.updates.parameters():\n",
        "              p.requires_grad = True\n",
        "            \n",
        "            logits = model(sents, sent_lens, preds, heads, rels, arg_begin_end, updates=True)\n",
        "\n",
        "          # Normal step\n",
        "          if not stop_training_lower:\n",
        "            if args['use_updates']:\n",
        "              for p in model.parameters():\n",
        "                p.requires_grad = True\n",
        "              for p in model.updates.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "            logits = model(sents, sent_lens, preds, heads, rels, arg_begin_end, updates=False)\n",
        "            # if b == n_train_batches - 1:\n",
        "            #   print(logits[0])\n",
        "          \n",
        "          # else: # Not attention\n",
        "          #   logits = model(sents, sent_lens, preds, heads, rels, arg_begin_end)\n",
        "\n",
        "          loss = bce_loss(logits, labels)\n",
        "          loss.backward()\n",
        "          if model.eye and stop_training_lower:\n",
        "            for name, param in model.named_parameters():\n",
        "              if 'updates' in name and 'weight' in name:\n",
        "                param.grad *= (torch.ones(param.data.shape) - torch.eye(param.data.shape[0])).to(device)\n",
        "                break\n",
        "\n",
        "          opt.step()\n",
        "          steps_ += 1\n",
        "          cma = (loss.item() + (steps_-1) * cma) / steps_\n",
        "          training_losses.append(cma)\n",
        "        # end of batch loop\n",
        "        steps += steps_\n",
        "\n",
        "        if not args['train_jointly']:\n",
        "          y_dev = get_prop_specific_labels(y['dev'], prop_i)\n",
        "          predictions = get_test_predictions(model, X['dev'], y_dev, updates=(phase_marker != 0))\n",
        "          results, metrics = evaluate(args, predictions, y_dev)\n",
        "        else:\n",
        "          predictions = get_test_predictions(model, X['dev'], y['dev'], updates=(phase_marker != 0))\n",
        "          results, metrics = evaluate(args, predictions, y['dev'])\n",
        "\n",
        "        F, precision, recall = metrics['F'], metrics['precision'], metrics['recall']\n",
        "        #dev_losses.append(np.mean(dev_loss))\n",
        "        dev_losses.append(-1)\n",
        "\n",
        "        print(f\"Epoch {e}, F={F*100:.2f}, p={precision*100:.2f}, r={recall*100:.2f}\")\n",
        "\n",
        "        if F > prev_best:\n",
        "          prev_best = F\n",
        "          torch.save(model.state_dict(), weights_path)\n",
        "\n",
        "        if args['show_weights'] and stop_training_lower and e % 5 == 0:\n",
        "          for name, param in model.named_parameters():\n",
        "            if 'updates' in name and 'weight' in name:\n",
        "              updates_weights = param\n",
        "          with torch.no_grad():\n",
        "            #show_weights((updates_weights - original_updates_weights).cpu(), properties=model.properties)\n",
        "            show_weights((updates_weights).cpu(), properties=model.properties)\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    # End of train loop\n",
        "\n",
        "    model.load_state_dict(torch.load(weights_path)) # Get best weights back\n",
        "\n",
        "    return model, training_losses, dev_losses, steps\n",
        "\n",
        "\n",
        "def show_weights(weights, properties=None, cmap='RdBu'):\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    norm = None\n",
        "    if cmap == 'RdBu':\n",
        "      vmax = weights.max()\n",
        "      vmin = weights.min()\n",
        "      if type(weights) == type(torch.randn(1)):\n",
        "        vmax = vmax.item()\n",
        "        vmin = vmin.item()\n",
        "      norm = MidpointNormalize(vmin=vmin, vmax=vmax, midpoint=0)\n",
        "\n",
        "    ax.set_xticks(np.arange(len(properties)))\n",
        "    ax.set_yticks(np.arange(len(properties)))\n",
        "    ax.set_xticklabels(properties)\n",
        "    ax.set_yticklabels(properties)\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode=\"anchor\")\n",
        "\n",
        "    if norm != None:\n",
        "      plt.imshow(weights, cmap=cmap, norm=norm)\n",
        "    else:\n",
        "      plt.imshow(weights, cmap=cmap)\n",
        "    cbar = plt.colorbar()\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def bce_loss(logits, labels):\n",
        "    # Expected labels : (B, num_properties)\n",
        "    loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def data_loader(X, y, batch_size=None, shuffle_idx=False):\n",
        "    data = list(zip(X, y))\n",
        "    idx = list(range(len(data)))\n",
        "    while True:\n",
        "        if shuffle_idx:\n",
        "            random.shuffle(idx) # In-place shuffle\n",
        "        \n",
        "        for span in idx_spans(idx, batch_size):\n",
        "            batch = [data[i] for i in span]\n",
        "            yield prepare_batch(batch)\n",
        "\n",
        "\n",
        "def idx_spans(idx, span_size):\n",
        "    for i in range(0, len(idx), span_size):\n",
        "        yield idx[i:i+span_size]\n",
        "\n",
        "\n",
        "def prepare_batch(batch):\n",
        "    # batch[i] = X, y\n",
        "    batch_size = len(batch)\n",
        "    sent_lens = torch.LongTensor([len(x[0][0]) for x in batch])\n",
        "    max_length = torch.max(sent_lens).item()\n",
        "    n_properties = len(batch[0][1])\n",
        "\n",
        "    # Zero is padding index\n",
        "    sents = torch.zeros((batch_size, max_length)).long().to(device)\n",
        "    preds = torch.zeros(batch_size).long().to(device)\n",
        "    heads = torch.zeros(batch_size).long().to(device)\n",
        "    rels = torch.zeros(batch_size).long().to(device)\n",
        "    arg_begin_end = torch.zeros(batch_size, 2).long().to(device)\n",
        "    labels = torch.zeros(batch_size, n_properties).to(device)\n",
        "\n",
        "    for i, (X_batch, y_batch) in enumerate(batch):\n",
        "        sent, (pred_idx, head_idx), head_rel_i, (arg_begin, arg_end) = X_batch\n",
        "        sents[i,:len(sent)] = torch.LongTensor(sent)\n",
        "        preds[i] = pred_idx\n",
        "        heads[i] = head_idx\n",
        "        rels[i] = head_rel_i\n",
        "        arg_begin_end[i] = torch.LongTensor([arg_begin, arg_end])\n",
        "        labels[i] = torch.tensor(y_batch)\n",
        "\n",
        "    return sents, sent_lens, preds, heads, rels, arg_begin_end, labels\n",
        "\n",
        "\n",
        "            #old dev eval code\n",
        "          # with torch.no_grad():\n",
        "          #   model.eval()\n",
        "          #   tp = 0\n",
        "          #   fp = 0\n",
        "          #   fn = 0\n",
        "          #   dev_loss = 0\n",
        "          #   for b in tqdm(range(n_dev_batches), ascii=True, desc=f'Evaluating progress', ncols=80, position=0, leave=True):\n",
        "          #     sents, sent_lens, preds, heads, labels = next(loader_dev)\n",
        "          #     results_, metrics_, dev_loss_ = evaluate(args, model, sents, sent_lens, preds, heads, labels)\n",
        "          #     tp += results_['tp']\n",
        "          #     fp += results_['fp']\n",
        "          #     fn += results_['fn']\n",
        "          #     dev_loss += dev_loss_.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyxihDMkczmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def evaluate(args, model, sents, sent_lens, preds, heads, labels):\n",
        "#     # Get predictions\n",
        "#     logits = model(sents, sent_lens, preds, heads)\n",
        "#     dev_loss = bce_loss(logits, labels)\n",
        "#     predictions = model.predict(logits)\n",
        "\n",
        "#     predictions, labels = predictions.cpu().numpy(), labels.cpu().numpy()\n",
        "#     results, metrics = get_results_metrics(predictions, labels)\n",
        "\n",
        "#     return results, metrics, dev_loss\n",
        "\n",
        "\n",
        "def evaluate_with_prop_breakdown(args, predictions, labels):\n",
        "  #expecting predictions and labels to have shape (n_test, 18)\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  if labels.shape[1] != len(PROPERTIES):\n",
        "    print('Labels should be (n_test, 18) for a prop breakdown')\n",
        "    raise Exception\n",
        "  \n",
        "  prop_specific_metrics = {}\n",
        "  for prop_i, p in enumerate(PROPERTIES):\n",
        "    predictions_p = predictions[:, prop_i]\n",
        "    labels_p = labels[:, prop_i]\n",
        "\n",
        "    results, metrics = evaluate(args, predictions_p, labels_p)\n",
        "\n",
        "    prop_specific_metrics[p] = metrics\n",
        "  \n",
        "  return prop_specific_metrics\n",
        "\n",
        "\n",
        "def evaluate(args, predictions, labels):\n",
        "  # Get predictions\n",
        "  labels = np.array(labels)\n",
        "  predictions, labels = predictions.cpu().numpy(), labels\n",
        "  results, metrics = get_results_metrics(predictions, labels)\n",
        "\n",
        "  return results, metrics\n",
        "\n",
        "\n",
        "def get_results_metrics(predictions, labels):\n",
        "  n_correct = (predictions == labels).astype(int).sum()\n",
        "\n",
        "  # Precision, Recall\n",
        "  eq = predictions == labels\n",
        "  neq = predictions != labels\n",
        "\n",
        "  pos_preds = predictions == 1\n",
        "  neg_preds = predictions == 0\n",
        "\n",
        "  tp = np.where(pos_preds, eq, 0).astype(int).sum()\n",
        "  fp = np.where(pos_preds, neq, 0).astype(int).sum()\n",
        "  fn = np.where(neg_preds, neq, 0).astype(int).sum()\n",
        "  \n",
        "  results = {\n",
        "          'tp': tp,\n",
        "          'fp': fp,\n",
        "          'fn': fn\n",
        "          }\n",
        "\n",
        "  F, precision, recall = F_precision_recall(tp, fp, fn)\n",
        "  metrics = {'F': F, 'precision': precision, 'recall': recall}\n",
        "\n",
        "  return results, metrics\n",
        "\n",
        "\n",
        "def F_precision_recall(tp, fp, fn):\n",
        "  if tp + fp > 0.:\n",
        "      precision = tp / (tp + fp)\n",
        "  else:\n",
        "      precision = 0.\n",
        "\n",
        "  if tp + fn > 0.:\n",
        "      recall = tp / (tp + fn)\n",
        "  else:\n",
        "      recall = 0.\n",
        "\n",
        "  if precision + recall > 0.:\n",
        "      F = (2 * precision * recall) / (precision + recall)\n",
        "  else:\n",
        "      F = 0.\n",
        "\n",
        "  return F, precision, recall\n",
        "\n",
        "\n",
        "def micro_average(results):\n",
        "  tp, fp, fn = 0, 0, 0\n",
        "  for v in results.values():\n",
        "      tp += v['tp']\n",
        "      fp += v['fp']\n",
        "      fn += v['fn']\n",
        "  \n",
        "  return F_precision_recall(tp, fp, fn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec5YvM4uHvsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(args):\n",
        "    df = pd.read_csv(PROTO_TSV, sep='\\t')\n",
        "\n",
        "    # Sentences\n",
        "    sent_ids = set(df['Sentence.ID'].tolist())\n",
        "    sents_path = os.path.join(PICKLED_DIR, 'sents.pkl')\n",
        "    sents = None\n",
        "    with open(sents_path, 'rb') as f:\n",
        "      sents = pickle.load(f)\n",
        "\n",
        "    # Dependency data\n",
        "    # dependencies_path = os.path.join(PICKLED_DIR, 'dependencies.pkl')\n",
        "    # with open(dependencies_path, 'rb') as f:\n",
        "    #     deps, deps_just_tokens = pickle.load(f)  \n",
        "    # sents['dependencies'] = deps\n",
        "    # sents['deps_just_tokens'] = deps_just_tokens\n",
        "\n",
        "\n",
        "    # Instances\n",
        "    path = os.path.join(PICKLED_DIR, 'instances.pkl')\n",
        "    proto_instances = None\n",
        "    possible = None # Data to compare to SPRL paper\n",
        "    with open(path, 'rb') as f:\n",
        "      proto_instances, possible = pickle.load(f)\n",
        "\n",
        "    # Word embedding data\n",
        "    w2e = None\n",
        "    path = os.path.join(PICKLED_DIR, f\"glove_{args['glove_d']}.pkl\")\n",
        "    with open(path, 'rb') as f:\n",
        "      w2e = pickle.load(f)\n",
        "\n",
        "    w2i, i2w = None, None\n",
        "    emb_np = None\n",
        "    X, y = None, None\n",
        "    dicts_path = os.path.join(PICKLED_DIR, 'dicts.pkl')\n",
        "    with open(dicts_path, 'rb') as f:\n",
        "        w2i, i2w = pickle.load(f)\n",
        "    \n",
        "    emb_np_path = os.path.join(PICKLED_DIR, 'emb_np.pkl')\n",
        "    with open(emb_np_path, 'rb') as f:\n",
        "        emb_np = pickle.load(f)\n",
        "    \n",
        "    #lstm_data_path = os.path.join(PICKLED_DIR, 'lstm_data.pkl')\n",
        "    lstm_data_path = os.path.join(PICKLED_DIR, 'lstm_data_modified.pkl')\n",
        "    with open(lstm_data_path, 'rb') as f:\n",
        "        X, y = pickle.load(f)\n",
        "\n",
        "    return {'df': df, \n",
        "            'proto_instances': proto_instances, \n",
        "            'possible': possible,\n",
        "            'sents': sents,\n",
        "            'w2e': w2e,\n",
        "            'sent_ids': sent_ids,\n",
        "            'lstm_data': (X,y),\n",
        "            'dicts': (w2i, i2w),\n",
        "            'emb_np': emb_np}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNVWhg68C0iN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From stackoverflow: https://stackoverflow.com/questions/7404116/defining-the-midpoint-of-a-colormap-in-matplotlib\n",
        "\n",
        "class MidpointNormalize(mpl.colors.Normalize):\n",
        "    def __init__(self, vmin, vmax, midpoint=0, clip=False):\n",
        "        self.midpoint = midpoint\n",
        "        mpl.colors.Normalize.__init__(self, vmin, vmax, clip)\n",
        "\n",
        "    def __call__(self, value, clip=None):\n",
        "        normalized_min = max(0, 1 / 2 * (1 - abs((self.midpoint - self.vmin) / (self.midpoint - self.vmax))))\n",
        "        normalized_max = min(1, 1 / 2 * (1 + abs((self.vmax - self.midpoint) / (self.midpoint - self.vmin))))\n",
        "        normalized_mid = 0.5\n",
        "        x, y = [self.vmin, self.midpoint, self.vmax], [normalized_min, normalized_mid, normalized_max]\n",
        "        return sp.ma.masked_array(sp.interp(value, x, y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1On2T0rcKKwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bootstrap_conf_interval(predictions, labels, B=10000, predictions_inferior=None):\n",
        "  predictions = predictions.cpu().numpy() # (n_test,18)\n",
        "\n",
        "  paired_test = False\n",
        "  if predictions_inferior != None:\n",
        "    paired_test = True\n",
        "    predictions_inferior = predictions_inferior.cpu().numpy()\n",
        "\n",
        "  N = predictions.shape[0]\n",
        "  indices = np.arange(N)\n",
        "  \n",
        "  labels = np.array(labels)\n",
        "\n",
        "  bs_samples = []\n",
        "  for b in tqdm(range(B), desc='Getting bootstrap samples', ncols=80, position=0, leave=True):\n",
        "    idx = np.random.choice(indices, N)\n",
        "    curr_preds = predictions[idx]\n",
        "    curr_labels = labels[idx]\n",
        "    results, metrics = get_results_metrics(curr_preds, curr_labels)\n",
        "    micro_f1 = metrics['F']\n",
        "\n",
        "    if paired_test:\n",
        "      curr_preds = predictions_inferior[idx]\n",
        "      curr_labels = labels[idx]\n",
        "      results, metrics = get_results_metrics(curr_preds, curr_labels)\n",
        "      micro_f1_inferior = metrics['F']\n",
        "\n",
        "      bs_samples.append(micro_f1 - micro_f1_inferior)\n",
        "    else:\n",
        "      bs_samples.append(micro_f1)\n",
        "  \n",
        "  bs_samples = np.array(bs_samples)\n",
        "\n",
        "  if paired_test:\n",
        "    prob_greater = 1. - np.mean(bs_samples > 0.)\n",
        "    print(f'P > : {prob_greater}')\n",
        "\n",
        "  plt.hist(bs_samples)\n",
        "  \n",
        "  return np.quantile(bs_samples, [0.025, 0.975])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7fJuKWbmsJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_predictions(model, X_test, y_test, batch_size=100, updates=True):\n",
        "\n",
        "  loader_test = data_loader(X_test, y_test, batch_size=batch_size, shuffle_idx=False)\n",
        "\n",
        "  n_batches = math.ceil(len(X_test) / batch_size)\n",
        "\n",
        "  predictions = []\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for b in tqdm(range(n_batches), ascii=True, desc=f'Getting test predictions', ncols=80, position=0, leave=True):\n",
        "      sents, sent_lens, preds, heads, rels, arg_begin_end, _ = next(loader_test)\n",
        "      logits = model(sents, sent_lens, preds, heads, rels, arg_begin_end, updates=updates)\n",
        "      predictions.append(model.predict(logits))\n",
        "  \n",
        "  predictions = torch.cat(predictions, dim=0)\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erm6Foa6jWJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def co_occurrences(y_train, normalize=False):\n",
        "  n_props = y_train[0].shape[0]\n",
        "\n",
        "  co_occur = np.zeros((n_props, n_props))\n",
        "  anti_occur = np.zeros((n_props, n_props))\n",
        "  both_negative = np.zeros((n_props, n_props))\n",
        "  row_pos_col_neg = np.zeros((n_props, n_props))\n",
        "  row_neg_col_pos = np.zeros((n_props, n_props))\n",
        "\n",
        "  positive_counts = np.zeros(n_props)\n",
        "  for labels in y_train:\n",
        "    for i in range(n_props):\n",
        "      if labels[i] == 1:\n",
        "        positive_counts[i] += 1\n",
        "        for j in range(i+1, n_props):\n",
        "          if labels[j] == 1:\n",
        "            co_occur[i,j] += 1\n",
        "\n",
        "        for j in range(n_props):\n",
        "          if labels[j] == 0:\n",
        "            row_pos_col_neg[i,j] += 1\n",
        "        \n",
        "      else: # labels[i] == 0\n",
        "        for j in range(i+1, n_props):\n",
        "          if labels[j] == 0:\n",
        "            both_negative[i,j] += 1\n",
        "        \n",
        "        for j in range(n_props):\n",
        "          if labels[j] == 1:\n",
        "            row_neg_col_pos[i,j] += 1\n",
        "  \n",
        "  negative_counts = len(y_train) - positive_counts\n",
        "\n",
        "  # Mirror over diagonal\n",
        "  co_occur = co_occur + np.transpose(co_occur)\n",
        "  both_negative = both_negative + np.transpose(both_negative)\n",
        "  #anti_occur = anti_occur + np.transpose(anti_occur)\n",
        "\n",
        "  def normalize_rows(array):\n",
        "    array /= np.sum(array, axis=-1, keepdims=True)\n",
        "\n",
        "  positive_counts = np.array(positive_counts)\n",
        "\n",
        "  #Alt def in marginals\n",
        "  # marginals = positive_counts / positive_counts.sum()\n",
        "  # print(marginals)\n",
        "\n",
        "  #co_occur = co_occur + np.diag(positive_counts)\n",
        "\n",
        "  # for i, p_i in enumerate(PROPERTIES):\n",
        "  #   for j, p_j in enumerate(PROPERTIES):\n",
        "  #     if co_occur[i,j] == 0:\n",
        "  #       print(f'{p_i} never occurs with {p_j}')\n",
        "  # MORAL OF STORY: ONLY 0-COOCCURRENCES ARE WITH CREATED AND DESTROYED IN RELATION TO EXISTENCE RELATIONS\n",
        "\n",
        "  #co_occur = co_occur + np.diag(positive_counts)\n",
        "\n",
        "  normalizing_term = co_occur.sum()\n",
        "  pij = co_occur / normalizing_term\n",
        "\n",
        "  #Supposing pij is correct...\n",
        "  marginals = pij.sum(-1)\n",
        "\n",
        "  outer = marginals * np.expand_dims(marginals, -1)\n",
        "\n",
        "  div = pij / outer\n",
        "  np.fill_diagonal(div, 1)\n",
        "\n",
        "  pmi = np.log(div)\n",
        "\n",
        "  #print(f'Max of outer is is {np.max(outer)}')\n",
        "  print(f'Max of pmi is {np.max(pmi)}')\n",
        "  print(f'Min of pmi is {np.min(pmi)}')\n",
        "\n",
        "  #breakpoint()\n",
        "\n",
        "\n",
        "  #print(f'Now max of anti_occur is {np.max(anti_occur)}, compared to {np.max(negative_counts)}')\n",
        "  \n",
        "\n",
        "  return co_occur, both_negative, row_pos_col_neg, row_neg_col_pos, pmi\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gFE7vvQhI3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fetch_test_predictions(model_name: str):\n",
        "  predictions_path = os.path.join(PREDICTIONS_DIR, model_name)\n",
        "  with open(predictions_path, 'rb') as f:\n",
        "    all_test_predictions = pickle.load(f)\n",
        "  return all_test_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUbR-mhy-dRb",
        "colab_type": "code",
        "outputId": "9cc266a9-9813-43f0-ca6c-8e9da782f5f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "args = {\n",
        "    'epochs': 20,\n",
        "    'seed': 7,\n",
        "    'lr': 1e-3,\n",
        "    'batch_size': 100,\n",
        "    'glove_d': 300,\n",
        "    'model_name': 'repeat_logreg_sharedwithrelu',\n",
        "\n",
        "    'use_updates': False,\n",
        "    'show_weights': True,\n",
        "    'stop_training_lower': 30,\n",
        "    'eye':True,\n",
        "    'updates_are_logits':False,\n",
        "\n",
        "    'use_lstm': False,\n",
        "    'h_size': 300,\n",
        "    'train_jointly': True, # Train the lstm jointly\n",
        "\n",
        "    'use_shared_rep': True, # Only applies to non-attention net\n",
        "    'use_dep_rels': True,\n",
        "\n",
        "    'sent_avg':True # Makes sense for any non-lstm model\n",
        "}\n",
        "if not args['use_updates']:\n",
        "  args['stop_training_lower'] = args['epochs']\n",
        "  args['show_weights'] = False\n",
        "\n",
        "seed = args['seed']\n",
        "\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "data = get_data(args)\n",
        "\n",
        "w2i, i2w = data['dicts']\n",
        "emb_np = data['emb_np']\n",
        "X, y = data['lstm_data']\n",
        "\n",
        "# co_occur, both_negative, row_pos_col_neg, row_neg_col_pos, pmi = co_occurrences(y['train'], normalize=False)\n",
        "# show_weights(pmi, properties=PROPERTIES)\n",
        "# show_weights(co_occur, properties=PROPERTIES, cmap='Greens')\n",
        "# show_weights(co_occur + both_negative, properties=PROPERTIES, cmap='Greens')\n",
        "#show_weights((row_pos_col_neg + row_neg_col_pos) /2, properties=PROPERTIES, cmap='Reds')\n",
        "#show_weights(row_neg_col_pos, properties=PROPERTIES, cmap='Reds')\n",
        "# show_weights((co_occur + both_negative) - (row_pos_col_neg + row_neg_col_pos), properties=PROPERTIES)\n",
        "#show_weights(co_occur - (row_pos_col_neg + row_neg_col_pos), properties=PROPERTIES)\n",
        "#breakpoint()\n",
        "\n",
        "do_train = 1\n",
        "if do_train:\n",
        "  model = SPRL(\n",
        "      vocab_size=len(w2i),\n",
        "      emb_size=int(args['glove_d']),\n",
        "      h_size=args['h_size'],\n",
        "      #shared_size=args['shared_size'],\n",
        "      padding_idx=w2i[PAD_TOKEN],\n",
        "      emb_np=emb_np,\n",
        "      properties=PROPERTIES if args['train_jointly'] else ['one_prop'],\n",
        "      use_updates=args['use_updates'],\n",
        "      use_lstm=args['use_lstm'],\n",
        "      direction_feature=not args['use_lstm'],\n",
        "      train_jointly=args['train_jointly'],\n",
        "      eye=args['eye'],\n",
        "      updates_are_logits=args['updates_are_logits'],\n",
        "      sent_avg=False if args['use_lstm'] else args['sent_avg'],\n",
        "      use_shared_rep=args['use_shared_rep'],\n",
        "      use_dep_rels=args['use_dep_rels'])\n",
        "  model.to(device)\n",
        "\n",
        "  train(args, model, X, y)\n",
        "\n",
        "preds_sup_name = 'logreg_fullpower_shared'\n",
        "\n",
        "paired = 0\n",
        "individual_CIs = 0\n",
        "tables = 0\n",
        "\n",
        "preds_sup = fetch_test_predictions(preds_sup_name)\n",
        "preds_inf = fetch_test_predictions('logreg_fullpower_sharedbutnoRELU')\n",
        "\n",
        "if individual_CIs:\n",
        "  CI = bootstrap_conf_interval(preds_sup, y['dev'])\n",
        "  print(CI)\n",
        "  CI = bootstrap_conf_interval(preds_inf, y['dev'])\n",
        "  print(CI)\n",
        "\n",
        "if paired:\n",
        "  CI = bootstrap_conf_interval(preds_sup, y['dev'], predictions_inferior=preds_inf)\n",
        "  print(f'\\n Confidence interval : {CI}\\n')\n",
        "\n",
        "if tables:\n",
        "  prop_specific_metrics = evaluate_with_prop_breakdown(args, preds_sup, y['dev'])\n",
        "\n",
        "  for k, v in prop_specific_metrics.items():\n",
        "      array.append([f\"{v['F']*100:.2f}\", f\"{v['precision']*100:.2f}\", f\"{v['recall']*100:.2f}\"])\n",
        "\n",
        "  array = np.array(array)\n",
        "  columns = ['F1', 'Precision', 'Recall']\n",
        "  dfm = pd.DataFrame(array, index=PROPERTIES, columns=columns)\n",
        "  tables_path = os.path.join(DATA_DIR, 'tables.txt')\n",
        "  with open(tables_path, 'w') as f:\n",
        "    f.write(preds_sup_name)\n",
        "    f.write(dfm.to_latex())\n",
        "\n",
        "print('Done!')\n",
        "\n"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20 progress: 100%|######################| 78/78 [00:01<00:00, 56.69it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 61.39it/s]\n",
            "Epoch 2/20 progress:   6%|#4                     | 5/78 [00:00<00:01, 47.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, F=78.87, p=82.20, r=75.80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/20 progress: 100%|######################| 78/78 [00:01<00:00, 56.27it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 61.63it/s]\n",
            "Epoch 3/20 progress:   8%|#7                     | 6/78 [00:00<00:01, 54.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, F=80.79, p=84.39, r=77.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/20 progress: 100%|######################| 78/78 [00:01<00:00, 54.80it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 63.90it/s]\n",
            "Epoch 4/20 progress:   8%|#7                     | 6/78 [00:00<00:01, 55.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2, F=81.75, p=84.43, r=79.23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/20 progress: 100%|######################| 78/78 [00:01<00:00, 56.30it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 61.23it/s]\n",
            "Epoch 5/20 progress:   8%|#7                     | 6/78 [00:00<00:01, 49.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3, F=81.97, p=84.42, r=79.65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/20 progress: 100%|######################| 78/78 [00:01<00:00, 56.38it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 65.46it/s]\n",
            "Epoch 6/20 progress:   8%|#7                     | 6/78 [00:00<00:01, 52.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4, F=81.54, p=85.15, r=78.23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/20 progress: 100%|######################| 78/78 [00:01<00:00, 55.92it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 61.78it/s]\n",
            "Epoch 7/20 progress:   6%|#4                     | 5/78 [00:00<00:01, 44.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5, F=82.16, p=83.76, r=80.62\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/20 progress: 100%|######################| 78/78 [00:01<00:00, 55.78it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 61.46it/s]\n",
            "Epoch 8/20 progress:   8%|#7                     | 6/78 [00:00<00:01, 52.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6, F=82.31, p=83.76, r=80.91\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/20 progress: 100%|######################| 78/78 [00:01<00:00, 55.99it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 60.43it/s]\n",
            "Epoch 9/20 progress:   8%|#7                     | 6/78 [00:00<00:01, 56.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7, F=82.12, p=83.72, r=80.59\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/20 progress: 100%|######################| 78/78 [00:01<00:00, 57.00it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 63.87it/s]\n",
            "Epoch 10/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 53.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8, F=82.64, p=83.58, r=81.72\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10/20 progress: 100%|#####################| 78/78 [00:01<00:00, 55.56it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 65.41it/s]\n",
            "Epoch 11/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 55.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9, F=82.49, p=82.76, r=82.23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/20 progress: 100%|#####################| 78/78 [00:01<00:00, 55.40it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 61.28it/s]\n",
            "Epoch 12/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 56.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10, F=82.18, p=83.52, r=80.88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/20 progress: 100%|#####################| 78/78 [00:01<00:00, 56.51it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 65.90it/s]\n",
            "Epoch 13/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 53.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11, F=81.85, p=83.56, r=80.20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/20 progress: 100%|#####################| 78/78 [00:01<00:00, 56.45it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 59.56it/s]\n",
            "Epoch 14/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 56.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12, F=81.93, p=83.08, r=80.80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/20 progress: 100%|#####################| 78/78 [00:01<00:00, 52.87it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 50.00it/s]\n",
            "Epoch 15/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 54.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13, F=82.20, p=83.62, r=80.82\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/20 progress: 100%|#####################| 78/78 [00:01<00:00, 56.20it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 65.27it/s]\n",
            "Epoch 16/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 53.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14, F=81.40, p=83.45, r=79.45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16/20 progress: 100%|#####################| 78/78 [00:01<00:00, 57.07it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 61.18it/s]\n",
            "Epoch 17/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 56.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15, F=82.16, p=82.44, r=81.88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17/20 progress: 100%|#####################| 78/78 [00:01<00:00, 57.42it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 64.48it/s]\n",
            "Epoch 18/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 50.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16, F=82.04, p=82.16, r=81.92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/20 progress: 100%|#####################| 78/78 [00:01<00:00, 55.04it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 64.47it/s]\n",
            "Epoch 19/20 progress:   8%|#6                    | 6/78 [00:00<00:01, 57.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17, F=82.03, p=82.56, r=81.52\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19/20 progress: 100%|#####################| 78/78 [00:01<00:00, 55.48it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 64.47it/s]\n",
            "Epoch 20/20 progress:   6%|#4                    | 5/78 [00:00<00:01, 48.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18, F=81.64, p=83.99, r=79.41\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20/20 progress: 100%|#####################| 78/78 [00:01<00:00, 54.82it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 64.59it/s]\n",
            "Getting test predictions: 100%|#################| 10/10 [00:00<00:00, 65.71it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19, F=81.72, p=83.09, r=80.40\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}